from fastapi import APIRouter, HTTPException, Body
from typing import Dict, Any, List, Optional
import pymongo
from langchain_openai import ChatOpenAI
from langchain.prompts import PromptTemplate
from dotenv import load_dotenv
import os
from pydantic import BaseModel, Field
import datetime
import bson
from bson.objectid import ObjectId
load_dotenv()
openai_api_key = os.getenv("openai_api_key")

# MongoDB bağlantısı
mongo_client = pymongo.MongoClient("mongodb://localhost:27017/")
db = mongo_client["physical_therapy_questions"]

# LLM router
llm_router = APIRouter(prefix="/api/llm", tags=["llm"])
#MODEL
llm = ChatOpenAI(
    api_key=openai_api_key,
    model="gpt-4o-2024-08-06",
    temperature=0,
    max_tokens=2000
)
# Requests
class QuestionExplanationRequest(BaseModel):
    """Soru açıklaması için gerekli giriş verileri"""
    question_id: str = Field(..., description="question ID (string veya ObjectId)")
    student_answer: int = Field(..., description="Student answer index (1-5)")

# Responses
class QuestionExplanationResponse(BaseModel):
    """Response data for question explanation"""
    status: str = Field("success", description="Operation status")
    explanation: str = Field(..., description="Explanation generated by the LLM")
    question: str = Field(..., description="Question text")
    choices: List[str] = Field(..., description="List of choices")
    correct_answer: int = Field(..., description="Index of the correct answer (1-5)")
    student_answer: int = Field(..., description="Index of the student's answer (1-5)")
    is_correct: bool = Field(..., description="Whether the student's answer is correct")
    cached: bool = Field(False, description="Whether the explanation was retrieved from cache")

@llm_router.post(
    "/explain-answer",
    response_model=QuestionExplanationResponse,
    summary="Question and answer explanation",
    description="Returns a detailed explanation generated by the LLM for a question"
)
async def explain_answer(request: QuestionExplanationRequest):
    """
    Endpoint for question and answer explanation.
    First checks if an explanation exists in the database, otherwise gets it from the LLM.
    """
    try:
        # Find the question in MongoDB - first search in diagnosis_test
        collections = ["diagnosis_test", "exam_questions"]
        question = None
        
        for collection_name in collections:
            collection = db[collection_name]
            
            # Search by string ID
            question = collection.find_one({"_id": request.question_id})
            
            # If not found, try with ObjectId
            if not question:
                try:
                    obj_id = ObjectId(request.question_id)
                    question = collection.find_one({"_id": obj_id})
                except:
                    pass
            
            if question:
                break
        
        if not question:
            raise HTTPException(status_code=404, detail="Question not found")
        
        # Get question details
        question_text = question.get("Problem", question.get("problem", ""))
        choices = question.get("Choices", question.get("choices", []))
        correct_answer = question.get("Answer Key", question.get("answer_key"))
        
        if not choices:
            raise HTTPException(status_code=400, detail="Question choices not found")
        
        if correct_answer is None:
            raise HTTPException(status_code=400, detail="Correct answer not found")
        
        # Normalize answer indices (1-based)
        if isinstance(correct_answer, int) and correct_answer >= 1:
            correct_answer_index = correct_answer
        else:
            correct_answer_index = int(correct_answer) if str(correct_answer).isdigit() else 1
        
        student_answer_index = request.student_answer
        
        # Check correctness
        is_correct = student_answer_index == correct_answer_index
        
        explanation_text = ""
        cached = False
        
        # Explanations collection
        explanations_collection = db["question_explanations"]
        
        # Try to find explanation in cache
        if is_correct:
            # For correct answer: search only by question_id
            cache_query = {
                "question_id": request.question_id,
                "explanation_type": "correct_answer"
            }
            print(f"🔍 Searching correct answer cache: question_id={request.question_id}")
            
        else:
            # For wrong answer: search by question_id + student_answer combination
            cache_query = {
                "question_id": request.question_id,
                "explanation_type": "wrong_answer",
                "student_answer": student_answer_index
            }
            print(f"🔍 Searching wrong answer cache: question_id={request.question_id}, student_answer={student_answer_index}")
        
        # Check if exists in cache
        existing_explanation = explanations_collection.find_one(cache_query)
        
        if existing_explanation:
            explanation_text = existing_explanation.get("explanation", "")
            cached = True
            cache_type = "correct answer" if is_correct else f"wrong answer ({student_answer_index})"
            print(f"📚 Explanation retrieved from cache ({cache_type}): {request.question_id}")
        else:
            # Not in cache, get from LLM and save
            explanation_text = await generate_explanation(question_text, choices, correct_answer_index, student_answer_index, is_correct)
            
            # Save to database
            explanation_doc = {
                "question_id": request.question_id,
                "explanation": explanation_text,
                "explanation_type": "correct_answer" if is_correct else "wrong_answer",
                "created_at": datetime.datetime.utcnow(),
                "question_text": question_text,
                "correct_answer": correct_answer_index,
                "student_answer": student_answer_index,  # Important for wrong answers
                "usage_count": 1  # Track how many times used
            }
            
            explanations_collection.insert_one(explanation_doc)
            cache_type = "correct answer" if is_correct else f"wrong answer ({student_answer_index})"
            print(f"💾 New explanation saved to database ({cache_type}): {request.question_id}")
        
        # If retrieved from cache, increment usage count
        if cached:
            explanations_collection.update_one(
                cache_query,
                {"$inc": {"usage_count": 1}, "$set": {"last_used": datetime.datetime.utcnow()}}
            )
            print(f"📊 Cache usage count incremented")
        
        return QuestionExplanationResponse(
            explanation=explanation_text,
            question=question_text,
            choices=choices,
            correct_answer=correct_answer_index,
            student_answer=student_answer_index,
            is_correct=is_correct,
            cached=cached
        )
    
    except Exception as e:
        print(f"LLM explanation error: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error occurred while generating explanation: {str(e)}")
async def generate_explanation(question_text: str, choices: List[str], correct_answer_index: int, student_answer_index: int, is_correct: bool) -> str:
    """
    LLM ile açıklama oluştur
    """
    try:
        # Seçenekleri formatla (1-based)
        choices_text = ""
        for i, choice in enumerate(choices, 1):
            choices_text += f"{i}. {choice}\n"
        
        # Doğru ve öğrenci cevap metinlerini al
        correct_answer_text = choices[correct_answer_index - 1] if correct_answer_index <= len(choices) else "Unknown"
        student_answer_text = choices[student_answer_index - 1] if student_answer_index <= len(choices) else "Unknown"
        
        # LLM prompt oluştur (İngilizce prompt, Korece cevap)
        if is_correct:
            prompt_template = """
You are a medical education expert. The student answered correctly! Please provide a clear and concise explanation in Korean.

Question: {question}

Choices:
{choices}

Correct Answer: {correct_answer_index}. {correct_answer_text}
Student's Answer: {student_answer_index}. {student_answer_text}

Please explain in Korean:
1. Why this answer is correct

Provide the explanation in Korean language, keep it clear, short and educational. Do not provide any additional information or context. Only explanation in Korean.

Korean Explanation:"""
        else:
            prompt_template = """
You are a medical education expert. The student answered incorrectly. Please provide a clear and helpful explanation in Korean.

Question: {question}

Choices:
{choices}

Correct Answer: {correct_answer_index}. {correct_answer_text}
Student's Answer: {student_answer_index}. {student_answer_text}

Please explain in Korean:
1. Why the correct answer ({correct_answer_index}) is right
2. Why the student's choice ({student_answer_index}) is wrong
3. Common misconceptions that might lead to choosing the wrong answer

Provide the explanation in Korean language, be short, supportive and educational.Do not provide any additional information or context. Only explanation in Korean.

Korean Explanation:"""
        
        # LLM çağrısı
        prompt = PromptTemplate(
            template=prompt_template,
            input_variables=["question", "choices", "correct_answer_index", "correct_answer_text", 
                           "student_answer_index", "student_answer_text"]
        )
        
        chain = prompt | llm
        
        explanation = chain.invoke({
            "question": question_text,
            "choices": choices_text,
            "correct_answer_index": correct_answer_index,
            "correct_answer_text": correct_answer_text,
            "student_answer_index": student_answer_index,
            "student_answer_text": student_answer_text
        })
        
        # LangChain response'dan text çıkar
        explanation_text = explanation.content if hasattr(explanation, 'content') else str(explanation)
        
        return explanation_text.strip()
        
    except Exception as e:
        print(f"LLM explanation generation error: {str(e)}")
        return "죄송합니다. 설명을 생성하는 중 오류가 발생했습니다."